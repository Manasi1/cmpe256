{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaned Copy of CE 256 Project - Amazon Book Recommendations Using Amazon and Goodreads Reviews",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UctXIm51vSL",
        "outputId": "54838c90-2cf9-4f42-90d2-595c85ad98eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAkeWrGiYhdt"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z-QQHKBYXxC"
      },
      "source": [
        "**Training** - Train a model on the Amazon dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf4koE0jvx-Q"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C5nA-twaXZa",
        "outputId": "10002818-3715-4a40-9100-28757510f743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89EUBGh6Yr1q",
        "outputId": "2e3196d0-665d-422b-c727-1dce8d64db3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Get the first 4 million reviews from CSV named amazon_reviews_1 in Google Drive\n",
        "\n",
        "# Go to folder containing CSV\n",
        "%cd \"/content/drive/My Drive/data/ce256/project\"\n",
        "# load first 4 million reviews\n",
        "amazon_df = pd.read_csv(\"amazon_reviews_1\")\n",
        "\n",
        "# Check the CSV in Google Drive has the correct data\n",
        "amazon_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data/ce256/project\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>vote</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>03 30, 2005</td>\n",
              "      <td>A1REUF3A1YCPHM</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>TW Ervin II</td>\n",
              "      <td>The King, the Mice and the Cheese by Nancy Gur...</td>\n",
              "      <td>A story children will love and learn from</td>\n",
              "      <td>1112140800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>06 20, 2016</td>\n",
              "      <td>AVP0HXC9FG790</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>The kids loved it!</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1466380800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>01 24, 2016</td>\n",
              "      <td>A324TTUBKTN73A</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Tekla Borner</td>\n",
              "      <td>My students (3 &amp; 4 year olds) loved this book!...</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1453593600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>07 9, 2015</td>\n",
              "      <td>A2RE7WG349NV5D</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Deborah K Woroniecki</td>\n",
              "      <td>LOVE IT</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1436400000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>01 18, 2015</td>\n",
              "      <td>A32B7QIUDQCD0E</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>NaN</td>\n",
              "      <td>E</td>\n",
              "      <td>Great!</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1421539200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall  verified   reviewTime  ... unixReviewTime vote image\n",
              "0      5.0     False  03 30, 2005  ...     1112140800  NaN   NaN\n",
              "1      5.0      True  06 20, 2016  ...     1466380800  NaN   NaN\n",
              "2      5.0      True  01 24, 2016  ...     1453593600  NaN   NaN\n",
              "3      5.0     False   07 9, 2015  ...     1436400000  NaN   NaN\n",
              "4      5.0      True  01 18, 2015  ...     1421539200  NaN   NaN\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jit-bP_yxOsq",
        "outputId": "9fd94620-47d6-4705-cd1a-0289b591decc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Create ratings df containing only user ID, ASIN as book ID, and rating\n",
        "amazon_ratings_df = amazon_df[[\"reviewerID\", \"asin\", \"overall\"]].copy()\n",
        "amazon_ratings_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>overall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1REUF3A1YCPHM</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AVP0HXC9FG790</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A324TTUBKTN73A</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A2RE7WG349NV5D</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A32B7QIUDQCD0E</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       reviewerID        asin  overall\n",
              "0  A1REUF3A1YCPHM  0001713353      5.0\n",
              "1   AVP0HXC9FG790  0001713353      5.0\n",
              "2  A324TTUBKTN73A  0001713353      5.0\n",
              "3  A2RE7WG349NV5D  0001713353      5.0\n",
              "4  A32B7QIUDQCD0E  0001713353      5.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcGYmYxOb3kI",
        "outputId": "ea820949-3ea8-4c4d-8c55-c9d49a20bf6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Credits-Prof Eirinaki, Rashmi Sharma and Aditya Patel\n",
        "# conda install -c conda-forge scikit-surprise\n",
        "!pip install scikit-surprise\n",
        "from surprise import BaselineOnly\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise.model_selection.split import train_test_split\n",
        "from surprise.model_selection import cross_validate, GridSearchCV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, io\n",
        "from surprise import KNNBasic, KNNWithMeans\n",
        "from surprise import SVDpp\n",
        "from surprise import SVD\n",
        "from surprise import accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/37/5d334adaf5ddd65da99fc65f6507e0e4599d092ba048f4302fe8775619e8/scikit-surprise-1.1.1.tar.gz (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp36-cp36m-linux_x86_64.whl size=1670925 sha256=0fc3462a09b39d4128fe9497f9fd5af924d36aa7b91a0a15d1599faa1252c52d\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/9c/3d/41b419c9d2aff5b6e2b4c0fc8d25c538202834058f9ed110d0\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdK6lommb5v_"
      },
      "source": [
        "reader = Reader(rating_scale=(1,5))  #invoke reader instance of surprise library\n",
        "data=Dataset.load_from_df(amazon_ratings_df,reader) #load dataset into Surprise datastructure Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ARnhnCtiOV5"
      },
      "source": [
        "#create training set\n",
        "trainingSet, testSet = train_test_split(data, test_size=0.2, train_size=None, random_state=None, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa9Py3ltiR1M"
      },
      "source": [
        "#SVD\n",
        "svd = SVD()\n",
        "svd.fit(trainingSet) #fit model to the training set\n",
        "predictions_svd = svd.test(testSet) #predict for test set values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaAzeGIepHgF"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBygpY-QiVCf",
        "outputId": "9b786576-ec6e-4687-8936-0226dd13f39b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#validating rating predictions using RMSE\n",
        "accuracy.rmse(predictions_svd, verbose=True) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.9699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9698739953380167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMf6eZd4pLW-"
      },
      "source": [
        "Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1oOIVP6Dhxo"
      },
      "source": [
        "# Get predicted and actual ratings of a user in the dataset\n",
        "for pred in predictions_svd:\n",
        "  if pred[0] == \"AQEO3JYVJJH31\":\n",
        "    print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQxd8ZYc6Ai3",
        "outputId": "025e2d90-2c37-4f96-9eeb-7b493e69ba9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get example predicted ratings of books to recommend to the user\n",
        "amazon_user_id = \"AQEO3JYVJJH31\"\n",
        "svd.predict(amazon_user_id, \"0091944244\") # A Gentleman in Moscow, user already rated this 5.0\n",
        "svd.predict(amazon_user_id, \"0007548672\") # All the Light We Cannot See, user already rated this 5.0\n",
        "svd.predict(amazon_user_id, \"0001713353\") # The King, the Mice and the Cheese\n",
        "svd.predict(amazon_user_id, \"0001384198\") # The Little Engine that Could\n",
        "svd.predict(amazon_user_id, \"0002005263\") # The Sinister Pig\n",
        "svd.predict(amazon_user_id, \"059035342X\") # Harry Potter and the Sorcerer's Stone, not in dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(uid='AQEO3JYVJJH31', iid='059035342X', r_ui=None, est=4.472453209445775, details={'was_impossible': False})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoYSHNNA9U82"
      },
      "source": [
        "\"\"\"\n",
        "Get the top-N highest-rated books as prediction tuples from the Amazon dataset.\n",
        "param: user_id: The ID of the reviewer who will be recommended books\n",
        "param: top_n: Number of books to recommend\n",
        "returns: List of N prediction tuples\n",
        "\"\"\"\n",
        "def get_recommended_amazon_books_by_asin(user_id, top_n):\n",
        "  # Get ASIN of all books as a list\n",
        "  asin_list = amazon_df[\"asin\"].tolist()\n",
        "  # Get ASIN list without duplicate ASINs\n",
        "  no_duplicates_asin_list = list( dict.fromkeys(asin_list) )\n",
        "  # print length to get number of unique ASINs\n",
        "  # print(len(no_duplicates_asin_list))\n",
        "\n",
        "  pred_list = []\n",
        "  for asin in no_duplicates_asin_list:\n",
        "    pred = svd.predict(user_id, asin)\n",
        "    pred_list.append(pred)\n",
        "  # Sort by the estimated rating, which is fourth element in pred tuple\n",
        "  # reverse=True to sort by highest ratings first\n",
        "  pred_list.sort(key=lambda x: x[3], reverse=True)\n",
        "  # Slice list to get top-N book preds\n",
        "  return pred_list[:top_n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukGyZBX9xXV6"
      },
      "source": [
        "# Now go through all books in Amazon df, and recommend the top-5 highest-rated books\n",
        "amazon_user_id = \"AQEO3JYVJJH31\"\n",
        "recommended_amazon_books = get_recommended_amazon_books_by_asin(amazon_user_id, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1WBAykd_3Q8",
        "outputId": "fc5bff3c-4d27-4d2a-cd1d-b993ed79deae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for pred in recommended_amazon_books:\n",
        "  print(\"ASIN: \" + str(pred[1]) + \", predicted rating: \" + str(pred[3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ASIN: 0001720279, predicted rating: 5\n",
            "ASIN: 0001720392, predicted rating: 5\n",
            "ASIN: 0001712845, predicted rating: 5\n",
            "ASIN: 0001983679, predicted rating: 5\n",
            "ASIN: 0001473727, predicted rating: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT8kNOmvtkr0"
      },
      "source": [
        "Alternatively: Making predictions using user and book dfs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCCVwZKp3s2F"
      },
      "source": [
        "# read the csv into a dataframe\n",
        "user_df = pd.read_csv(\"sample_user_name.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T-ibMui30R-",
        "outputId": "1ed75935-add2-4e35-f21e-77abc0baa4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "user_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user1</td>\n",
              "      <td>AQEO3JYVJJH31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  username             id\n",
              "0    user1  AQEO3JYVJJH31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQOcxdPo314L"
      },
      "source": [
        "user_dict = {}\n",
        "for i in range(len(user_df)):\n",
        "    user_dict[user_df.iloc[i].username] = user_df.iloc[i].id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2O8pTQdDEr-",
        "outputId": "3365faff-23b9-4a57-d5f1-fa4c6147faf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(user_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'user1': 'AQEO3JYVJJH31'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLfoWrle33gi"
      },
      "source": [
        "# read the csv into a dataframe\n",
        "book_df = pd.read_csv(\"sample_book_name.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QORu7gsa4AJ0",
        "outputId": "3f3314ef-8c96-474b-94e3-122810280e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "book_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bookName</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A Gentleman in Moscow</td>\n",
              "      <td>91944244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>All the Light We Cannot See</td>\n",
              "      <td>7548672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Anatomy of Peace: Resolving the Heart of C...</td>\n",
              "      <td>141047666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Complete Idiot's Guide to Music Theory</td>\n",
              "      <td>28643771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            bookName         id\n",
              "0                              A Gentleman in Moscow   91944244\n",
              "1                        All the Light We Cannot See    7548672\n",
              "2  The Anatomy of Peace: Resolving the Heart of C...  141047666\n",
              "3         The Complete Idiot's Guide to Music Theory   28643771"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmbwlIlJbpFw"
      },
      "source": [
        "\"\"\"Add leading zeros to get the correct 10-digit ASIN.\n",
        ":param book_df: The df containing a column called \"id\" which need to be 10-digit ASINs\n",
        "\"\"\"\n",
        "def add_leading_zeros_to_ids(book_df):\n",
        "  for id in book_df[\"id\"]:\n",
        "    # print(str(id).zfill(10))\n",
        "    ten_digit_id = str(id).zfill(10)\n",
        "    book_df[\"id\"] = book_df[\"id\"].replace([id], ten_digit_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WnqIZIvhNiv",
        "outputId": "dbc7ad1e-e88a-4e90-f2db-37dd91ee4564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "add_leading_zeros_to_ids(book_df)\n",
        "book_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bookName</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A Gentleman in Moscow</td>\n",
              "      <td>0091944244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>All the Light We Cannot See</td>\n",
              "      <td>0007548672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Anatomy of Peace: Resolving the Heart of C...</td>\n",
              "      <td>0141047666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Complete Idiot's Guide to Music Theory</td>\n",
              "      <td>0028643771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            bookName          id\n",
              "0                              A Gentleman in Moscow  0091944244\n",
              "1                        All the Light We Cannot See  0007548672\n",
              "2  The Anatomy of Peace: Resolving the Heart of C...  0141047666\n",
              "3         The Complete Idiot's Guide to Music Theory  0028643771"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcNhGpQV4J5A"
      },
      "source": [
        "book_dict = {}\n",
        "for i in range(len(book_df)):\n",
        "    book_dict[book_df.iloc[i].id] = book_df.iloc[i].bookName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASd5f_PTJxHq",
        "outputId": "c1ec39f9-abe5-4a03-e873-1dfeb34b62ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(book_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0091944244': 'A Gentleman in Moscow', '0007548672': 'All the Light We Cannot See', '0141047666': 'The Anatomy of Peace: Resolving the Heart of Conflict', '0028643771': \"The Complete Idiot's Guide to Music Theory\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdeRMZGw1zJ6"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def getBookRecommendations(topN=3):\n",
        "    top_recs = defaultdict(list)\n",
        "    # for uid, iid, true_r, est, _ in predictions: \n",
        "    for uid, iid, true_r, est, _ in predictions_svd: \n",
        "        top_recs[uid].append((iid, est))\n",
        "     \n",
        "    for uid, user_ratings in top_recs.items():\n",
        "        user_ratings.sort(key = lambda x: x[1], reverse = True)\n",
        "        top_recs[uid] = user_ratings[:topN]\n",
        "     \n",
        "    return top_recs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SnPJdNz3nbq"
      },
      "source": [
        "recommendations = getBookRecommendations(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68I2ZoqB4Uav"
      },
      "source": [
        "def getBookName(book_id):\n",
        "    if book_id not in book_dict:\n",
        "        return \"\"\n",
        "    b = book_dict[book_id]\n",
        "    return b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l7Lt1UU4dbX"
      },
      "source": [
        "def getBookRecommendationsForUser(userId, recommendations):\n",
        "    if userId not in user_dict:\n",
        "        print(\"User id is not present\")\n",
        "        return\n",
        "    u_id = user_dict[userId]\n",
        "    recommended_books = recommendations[u_id]\n",
        "    book_list = []\n",
        "    for book in recommended_books:\n",
        "        book_list.append((getBookName(book[0]),book[1]))\n",
        "    return book_list    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-bqnUIa4qcA",
        "outputId": "dab7cd1e-3109-4969-a6da-28cb27ad40b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# change to user ID\n",
        "getBookRecommendationsForUser('user1',recommendations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('All the Light We Cannot See', 4.857948206684828)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l2DUsaPZm8e"
      },
      "source": [
        "**Training** - Train a model on the Goodreads dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD95K_CrZm8t"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW86aAz-Zm8z",
        "outputId": "7711396f-f580-4d29-f070-4d82819ae3a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "808vDh_tZm83",
        "outputId": "02b99d95-e7bc-4a56-c448-4d746eae5125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "# Get the first 4 million reviews from CSV named goodreads_reviews_1 in Google Drive\n",
        "\n",
        "# Go to folder containing CSV\n",
        "%cd \"/content/drive/My Drive/data/ce256/project\"\n",
        "# load first 4 million reviews\n",
        "goodreads_df = pd.read_csv(\"goodreads_reviews_1.csv\")\n",
        "\n",
        "# Check the CSV in Google Drive has the correct data\n",
        "goodreads_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data/ce256/project\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_text</th>\n",
              "      <th>date_added</th>\n",
              "      <th>date_updated</th>\n",
              "      <th>read_at</th>\n",
              "      <th>started_at</th>\n",
              "      <th>n_votes</th>\n",
              "      <th>n_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>24375664</td>\n",
              "      <td>5cd416f3efc3f944fce4ce2db2290d5e</td>\n",
              "      <td>5</td>\n",
              "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
              "      <td>Fri Aug 25 13:55:02 -0700 2017</td>\n",
              "      <td>Mon Oct 09 08:55:59 -0700 2017</td>\n",
              "      <td>Sat Oct 07 00:00:00 -0700 2017</td>\n",
              "      <td>Sat Aug 26 00:00:00 -0700 2017</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>18245960</td>\n",
              "      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n",
              "      <td>5</td>\n",
              "      <td>This is a special book. It started slow for ab...</td>\n",
              "      <td>Sun Jul 30 07:44:10 -0700 2017</td>\n",
              "      <td>Wed Aug 30 00:00:26 -0700 2017</td>\n",
              "      <td>Sat Aug 26 12:05:52 -0700 2017</td>\n",
              "      <td>Tue Aug 15 13:23:18 -0700 2017</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>6392944</td>\n",
              "      <td>5e212a62bced17b4dbe41150e5bb9037</td>\n",
              "      <td>3</td>\n",
              "      <td>I haven't read a fun mystery book in a while a...</td>\n",
              "      <td>Mon Jul 24 02:48:17 -0700 2017</td>\n",
              "      <td>Sun Jul 30 09:28:03 -0700 2017</td>\n",
              "      <td>Tue Jul 25 00:00:00 -0700 2017</td>\n",
              "      <td>Mon Jul 24 00:00:00 -0700 2017</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>22078596</td>\n",
              "      <td>fdd13cad0695656be99828cd75d6eb73</td>\n",
              "      <td>4</td>\n",
              "      <td>Fun, fast paced, and disturbing tale of murder...</td>\n",
              "      <td>Mon Jul 24 02:33:09 -0700 2017</td>\n",
              "      <td>Sun Jul 30 10:23:54 -0700 2017</td>\n",
              "      <td>Sun Jul 30 15:42:05 -0700 2017</td>\n",
              "      <td>Tue Jul 25 00:00:00 -0700 2017</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>6644782</td>\n",
              "      <td>bd0df91c9d918c0e433b9ab3a9a5c451</td>\n",
              "      <td>4</td>\n",
              "      <td>A fun book that gives you a sense of living in...</td>\n",
              "      <td>Mon Jul 24 02:28:14 -0700 2017</td>\n",
              "      <td>Thu Aug 24 00:07:20 -0700 2017</td>\n",
              "      <td>Sat Aug 05 00:00:00 -0700 2017</td>\n",
              "      <td>Sun Jul 30 00:00:00 -0700 2017</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            user_id   book_id  ... n_votes  n_comments\n",
              "0  8842281e1d1347389f2ab93d60773d4d  24375664  ...      16           0\n",
              "1  8842281e1d1347389f2ab93d60773d4d  18245960  ...      28           1\n",
              "2  8842281e1d1347389f2ab93d60773d4d   6392944  ...       6           0\n",
              "3  8842281e1d1347389f2ab93d60773d4d  22078596  ...      22           4\n",
              "4  8842281e1d1347389f2ab93d60773d4d   6644782  ...       8           0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9vUasPvZm86",
        "outputId": "1b1d9f59-d25b-427c-e9d4-0a1118a231a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Create ratings df containing only user ID, Goodreads book ID, and rating\n",
        "goodreads_ratings_df = goodreads_df[[\"user_id\", \"book_id\", \"rating\"]].copy()\n",
        "goodreads_ratings_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>24375664</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>18245960</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>6392944</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>22078596</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
              "      <td>6644782</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            user_id   book_id  rating\n",
              "0  8842281e1d1347389f2ab93d60773d4d  24375664       5\n",
              "1  8842281e1d1347389f2ab93d60773d4d  18245960       5\n",
              "2  8842281e1d1347389f2ab93d60773d4d   6392944       3\n",
              "3  8842281e1d1347389f2ab93d60773d4d  22078596       4\n",
              "4  8842281e1d1347389f2ab93d60773d4d   6644782       4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCkB12Dvc0yU",
        "outputId": "6b150ff9-0c5b-44eb-977e-d37a501dfd40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Credits-Prof Eirinaki, Rashmi Sharma and Aditya Patel\n",
        "# conda install -c conda-forge scikit-surprise\n",
        "!pip install scikit-surprise\n",
        "from surprise import BaselineOnly\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise.model_selection.split import train_test_split\n",
        "from surprise.model_selection import cross_validate, GridSearchCV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, io\n",
        "from surprise import KNNBasic, KNNWithMeans\n",
        "from surprise import SVDpp\n",
        "from surprise import SVD\n",
        "from surprise import accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (0.17.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnjvwLmXc0yb"
      },
      "source": [
        "reader = Reader(rating_scale=(1,5))  #invoke reader instance of surprise library\n",
        "data=Dataset.load_from_df(goodreads_ratings_df,reader) #load dataset into Surprise datastructure Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7zjKSgjc0yd"
      },
      "source": [
        "#create training set\n",
        "trainingSet, testSet = train_test_split(data, test_size=0.2, train_size=None, random_state=None, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWCV3rLuc0yf"
      },
      "source": [
        "#SVD\n",
        "svd = SVD()\n",
        "svd.fit(trainingSet) #fit model to the training set\n",
        "predictions_svd = svd.test(testSet) #predict for test set values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75n6eEJSc0yh"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYCcotgfc0yi",
        "outputId": "e91a2bde-6ca1-4154-e165-4d90184c1a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#validating rating predictions using RMSE\n",
        "accuracy.rmse(predictions_svd, verbose=True) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 1.0804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0803740490580012"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcH3Vzttc0yj"
      },
      "source": [
        "Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aIxCUu4c0yl",
        "outputId": "5d81402a-251a-4f39-ee7d-2b997686429b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get eaxmple predicted ratings of a user in the dataset\n",
        "goodreads_user_id = \"8842281e1d1347389f2ab93d60773d4d\"\n",
        "svd.predict(goodreads_user_id, \"24375664\") # The Dark Forest (Remembrance of Earth’s Past, #2), user already rated this 5.0\n",
        "svd.predict(goodreads_user_id, \"18245960\") # The Three-Body Problem (Remembrance of Earth’s Past, #1), user already rated this 5.0\n",
        "svd.predict(goodreads_user_id, \"6392944\") # The Murder on the Links (Hercule Poirot, #2), user already rated this 3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(uid='8842281e1d1347389f2ab93d60773d4d', iid='6392944', r_ui=None, est=3.3522237304915175, details={'was_impossible': False})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvUumwSWD5k5"
      },
      "source": [
        "\"\"\"\n",
        "Get the top-N highest-rated books as prediction tuples from the Goodreads dataset.\n",
        "param: user_id: The ID of the reviewer who will be recommended books\n",
        "param: top_n: Number of books to recommend\n",
        "returns: List of N prediction tuples\n",
        "\"\"\"\n",
        "def get_recommended_goodreads_books_by_id(user_id, top_n):\n",
        "  # Get book ID of all books as a list\n",
        "  book_id_list = goodreads_df[\"book_id\"].tolist()\n",
        "  # Get book ID list without duplicate book IDs\n",
        "  no_duplicates_book_id_list = list( dict.fromkeys(book_id_list) )\n",
        "  # print length to get number of unique book IDs\n",
        "  print(len(no_duplicates_book_id_list))\n",
        "\n",
        "  pred_list = []\n",
        "  for book_id in no_duplicates_book_id_list:\n",
        "    pred = svd.predict(user_id, book_id)\n",
        "    pred_list.append(pred)\n",
        "  # Sort by the estimated rating, which is fourth element in pred tuple\n",
        "  # reverse=True to sort by highest ratings first\n",
        "  pred_list.sort(key=lambda x: x[3], reverse=True)\n",
        "  # Slice list to get top-N book preds\n",
        "  return pred_list[:top_n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mek4u8q1E0tu",
        "outputId": "b915e993-db41-4355-a385-1c7816e67afc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now go through all books in Goodreads df, and recommend the top-5 highest-rated books\n",
        "goodreads_user_id = \"8842281e1d1347389f2ab93d60773d4d\"\n",
        "recommended_goodreads_books = get_recommended_goodreads_books_by_id(goodreads_user_id, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "963125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RlWWU_nFNuB",
        "outputId": "da50cf32-4278-45b8-fa14-63e430a2fe9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for pred in recommended_goodreads_books:\n",
        "  print(\"Book ID: \" + str(pred[1]) + \", predicted rating: \" + str(pred[3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Book ID: 7126, predicted rating: 5\n",
            "Book ID: 2, predicted rating: 5\n",
            "Book ID: 7304203, predicted rating: 5\n",
            "Book ID: 23437291, predicted rating: 5\n",
            "Book ID: 23489258, predicted rating: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqCoLYcESBAn"
      },
      "source": [
        "---\n",
        "**Display** title and authors\n",
        "\n",
        "After getting final list of recommended books of an Amazon user, display title and authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCZRwOf5lYkE"
      },
      "source": [
        "Goodreads API Client is a Python wrapper around the Goodreads API.\n",
        "\n",
        "https://pypi.org/project/goodreads-api-client/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iHUxaQ4YcZu",
        "outputId": "707d24ac-82eb-4fb7-a23d-97f9923a8ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        }
      },
      "source": [
        "# install to use Goodreads API\n",
        "!pip install goodreads_api_client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting goodreads_api_client\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/03/c7c9e027761d382a92c0cb4acf3bf1650ac2e844fd55851e08ccf950687a/goodreads_api_client-0.1.0.dev4-py2.py3-none-any.whl\n",
            "Collecting requests==2.18.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/92/c35ed010e8f96781f08dfa6d9a6a19445a175a9304aceedece77cd48b68f/requests-2.18.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.6MB/s \n",
            "\u001b[?25hCollecting xmltodict==0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a9/7e99652c6bc619d19d58cdd8c47560730eb5825d43a7e25db2e1d776ceb7/xmltodict-0.11.0-py2.py3-none-any.whl\n",
            "Collecting rauth==0.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/43/aa/7c8e852275394d65ac5bf3ac9945ecaafe4d083089e09cb0a267efea389a/rauth-0.7.3.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.3->goodreads_api_client) (3.0.4)\n",
            "Collecting idna<2.6,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/7d/9bbbd7bb35f34b0169542487d2a8859e44306bb2e6a4455d491800a5621f/idna-2.5-py2.py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[?25hCollecting urllib3<1.23,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.3->goodreads_api_client) (2020.6.20)\n",
            "Building wheels for collected packages: rauth\n",
            "  Building wheel for rauth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rauth: filename=rauth-0.7.3-cp36-none-any.whl size=16055 sha256=fe1e6cd2bd3f0f6348b7bfbf5d1423e01985d3f12718b09829db70b51c8450ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/94/5d/81afc278dd5da884a0002563dc4b0fe85f9067a5a40f76f858\n",
            "Successfully built rauth\n",
            "\u001b[31mERROR: tensorflow-datasets 4.0.1 has requirement requests>=2.19.0, but you'll have requests 2.18.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.3.0 has requirement requests<3,>=2.21.0, but you'll have requests 2.18.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas-datareader 0.9.0 has requirement requests>=2.19.0, but you'll have requests 2.18.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.18.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: idna, urllib3, requests, xmltodict, rauth, goodreads-api-client\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed goodreads-api-client-0.1.0.dev4 idna-2.5 rauth-0.7.3 requests-2.18.3 urllib3-1.22 xmltodict-0.11.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "requests",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8IvkzdVYsZl"
      },
      "source": [
        "import goodreads_api_client as gr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f652oAKrlVic"
      },
      "source": [
        "  # Set client API key\n",
        "  api_key = 'afbTRMOw7ZbCHlQS4kDRQ'\n",
        "  client = gr.Client(developer_key=api_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GScjfSC1l_zr"
      },
      "source": [
        "\"\"\"Uses Goodreads API Python client and an ISBN\n",
        "and returns Goodreads ID, title, and ISBN of book with that ID.\n",
        "\n",
        ":param client: The Goodreads API Python client\n",
        ":param isbn: The ISBN this book should have\n",
        ":returns: Dictionary with the Goodreads ID, title, and ISBN of the book\n",
        "\"\"\"\n",
        "def get_title_with_goodreads_api_and_isbn(client, isbn):\n",
        "  book = client.Book.show_by_isbn(isbn)\n",
        "  keys_wanted = ['id', 'title', 'isbn']\n",
        "  reduced_book = {k:v for k, v in book.items() if k in keys_wanted}\n",
        "  print(reduced_book)\n",
        "  return reduced_book"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPXfj8QHZh_w"
      },
      "source": [
        "\"\"\"Uses Goodreads API Python client and a Goodreads ID\n",
        "and returns Goodreads ID, title, and ISBN of book with that ID.\n",
        "\n",
        ":param client: The Goodreads API Python client\n",
        ":param id: the Goodreads ID this book should have\n",
        ":returns: Dictionary with the Goodreads ID, title, and ISBN of the book\n",
        "\"\"\"\n",
        "def get_title_with_goodreads_api(client, id):\n",
        "  book = client.Book.show(id)\n",
        "  keys_wanted = ['id', 'title', 'isbn']\n",
        "  reduced_book = {k:v for k, v in book.items() if k in keys_wanted}\n",
        "  print(reduced_book)\n",
        "  return reduced_book"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzSjDBmgp8ih"
      },
      "source": [
        "\"\"\"Uses Goodreads API Python client and a Goodreads ID\n",
        "and returns authors of book with that ID.\n",
        "\n",
        ":param client: The Goodreads API Python client\n",
        ":param id: the Goodreads ID this book should have\n",
        ":returns: Dictionary with the authors of the book\n",
        "\"\"\"\n",
        "def get_authors_with_goodreads_api(client, id):\n",
        "  book = client.Book.show(id)\n",
        "  keys_wanted = ['authors']\n",
        "  reduced_book = {k:v for k, v in book.items() if k in keys_wanted}\n",
        "  for item in reduced_book[\"authors\"].items():\n",
        "    print(item)\n",
        "  return reduced_book"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLPModipKl8L",
        "outputId": "3a22e858-523a-4bcc-ad0a-65b3aff2034d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the titles of recommended Amazon books using client and ASIN\n",
        "import time\n",
        "\n",
        "for pred in recommended_amazon_books:\n",
        "  get_title_with_goodreads_api_and_isbn(client, pred[1])\n",
        "  # Sleep 1 second to not go above max API requests\n",
        "  time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': '21083498', 'title': 'Hooray for Diffendoofer Day!', 'isbn': '0001720279'}\n",
            "{'id': '2711294', 'title': 'Green Eggs and Ham', 'isbn': '0001720392'}\n",
            "{'id': '668981', 'title': 'The Berenstain Bears and the Spooky Old Tree', 'isbn': '0001712845'}\n",
            "{'id': '421569', 'title': 'The Complete Brambly Hedge (Brambly Hedge, #1-8)', 'isbn': '0001983679'}\n",
            "{'id': '5935634', 'title': 'The Greatest Book on \"Dispensational Truth\" in the World', 'isbn': '0001473727'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpzVrxZjI9o8",
        "outputId": "f9c4b56e-3bf4-4a6b-c408-7ec1580a2523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the titles of recommended Goodreads books using client and Goodreads book ID\n",
        "import time\n",
        "\n",
        "for pred in recommended_goodreads_books:\n",
        "  get_title_with_goodreads_api(client, pred[1])\n",
        "  time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': '7126', 'title': 'The Count of Monte Cristo', 'isbn': '0140449264'}\n",
            "{'id': '2', 'title': 'Harry Potter and the Order of the Phoenix (Harry Potter, #5)', 'isbn': '0439358078'}\n",
            "{'id': '7304203', 'title': 'Shadowfever (Fever, #5)', 'isbn': '0385341679'}\n",
            "{'id': '23437291', 'title': 'Aflame (Fall Away, #4)', 'isbn': '0698403878'}\n",
            "{'id': '23489258', 'title': 'Second Debt (Indebted, #3)', 'isbn': '1507628552'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mauwQGrJr0Cr"
      },
      "source": [
        "# time.sleep(1)\n",
        "# get_authors_with_goodreads_api(client, \"24375664\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BYPfqtONM_s"
      },
      "source": [
        "The model should know which users and books are unique. This is to make sure the model is not accidentally recommending the same book from Goodreads that a user has reviewed on Amazon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YOYSfh6n3rd"
      },
      "source": [
        "The model will assume all users on Amazon are \n",
        "different from all users on Goodreads.\n",
        "That is, all user IDs are unique. There is no user with both an Amazon and Goodreads ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPocqKWUqAQX"
      },
      "source": [
        "To know which books are unique, we can check the book title and authors. If two books have the same title and authors, they are the same book.\n",
        "\n",
        "- The Amazon ASIN of a book is the same as the book's 10-digit ISBN (International Standard Book Number). \n",
        "- Can use Goodreads API and ISBN to get book title and author.\n",
        " \n",
        "See links:\n",
        "- https://www.oreilly.com/library/view/amazon-hacks/0596005423/ch01s03.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb5S9ObEAXp3"
      },
      "source": [
        "- Can use Goodreads API and Goodreads book ID to get title and author.\n",
        "See links:\n",
        "  - https://pypi.org/project/Goodreads/\n",
        "  - https://www.goodreads.com/api\n",
        "  - https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/books\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjX_JyvObDTX"
      },
      "source": [
        "# Test using Goodreads API on a random Goodreads book ID\n",
        "get_title_with_goodreads_api(client, \"1\")\n",
        "get_title_with_goodreads_api(client, \"4986701\")\n",
        "# Test using Goodreads API on a random ISBN\n",
        "get_title_with_goodreads_api_and_isbn(client, \"0001932349\")\n",
        "get_title_with_goodreads_api_and_isbn(client, \"0002005263\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bPTvAqVbbYN"
      },
      "source": [
        "Find Goodreads **reviews** similar to an Amazon user's review. Books that have reviews similar to the Amazon review can also be recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW2imKJVLMlC"
      },
      "source": [
        "https://dev.to/coderasha/compare-documents-similarity-using-python-nlp-4odp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8PaSloIku1v",
        "outputId": "ead95228-ca40-450f-d320-c6c19f22ec58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Make sure dataframe displays full review text\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8msjgRalc_NZ",
        "outputId": "709add97-afe1-470d-cdd3-618838cc0af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get one review of an Amazon user\n",
        "amazon_reviewer_id = \"AQEO3JYVJJH31\"\n",
        "all_reviews_of_user = amazon_df.loc[amazon_df[\"reviewerID\"] == amazon_reviewer_id]\n",
        "# Get the user's first review in the df\n",
        "first_review = all_reviews_of_user.iloc[0]\n",
        "first_review\n",
        "first_review_text = first_review[\"reviewText\"]\n",
        "print(first_review_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thoroughly good read, gives the perspective of the war from many angles, especially the impact on children.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iovaMnMflN97"
      },
      "source": [
        "# Get the first n reviews in the dataset as a list\n",
        "def get_goodreads_review_texts(n):\n",
        "  review_list = []\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    # Get one review from Goodreads dataset\n",
        "    goodreads_review = goodreads_df.iloc[i]\n",
        "    goodreads_review_text = goodreads_review[\"review_text\"]\n",
        "    review_list.append(goodreads_review_text)\n",
        "    # print(goodreads_review_text)\n",
        "    # print(\"--------------------\")\n",
        "    i += 1\n",
        "  return review_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I_dK8j3JnvN",
        "outputId": "7bf21864-1b01-42d5-dd14-8d784d0ccde8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the first 5 review texts of Goodreads dataset\n",
        "goodreads_review_texts = get_goodreads_review_texts(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mind blowingly cool. Best science fiction I've read in some time. I just loved all the descriptions of the society of the future - how they lived in trees, the notion of owning property or even getting married was gone. How every surface was a screen. \n",
            " The undulations of how society responds to the Trisolaran threat seem surprising to me. Maybe its more the Chinese perspective, but I wouldn't have thought the ETO would exist in book 1, and I wouldn't have thought people would get so over-confident in our primitive fleet's chances given you have to think that with superior science they would have weapons - and defenses - that would just be as rifles to arrows once were. \n",
            " But the moment when Luo Ji won as a wallfacer was just too cool. I may have actually done a fist pump. Though by the way, if the Dark Forest theory is right - and I see no reason why it wouldn't be - we as a society should probably stop broadcasting so much signal out into the universe.\n",
            "--------------------\n",
            "This is a special book. It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind. This is what I love about good science fiction - it pushes your thinking about where things can go. \n",
            " It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I've read. For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc. \n",
            " It is a book about science, and aliens. The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell. Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though. \n",
            " But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned? That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together. \n",
            " I did like how the book unveiled the Trisolaran culture through the game. It was a smart way to build empathy with them and also understand what they've gone through across so many centuries. And who know a 3 body problem was an unsolvable math problem? But I still don't get who made the game - maybe that will come in the next book. \n",
            " I loved this quote: \n",
            " \"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists? How many neutrons and electrons? Probably no fewer than a hundred million. Every collision was probably the end of the civilizations and intelligences in a microcosmos. In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons. Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"\n",
            "--------------------\n",
            "I haven't read a fun mystery book in a while and not sure I've ever read Poirot. Was looking for a fun read set in France while I was on holiday there and this didn't disappoint! Fast paced and good mystery. \n",
            " One that struck me was how similar Poirot is to Sherlock. They are both detectives, have a ex-military sidekick who is telling the story, and solve mysteries using their superior wit. Poirot seems like a French Sherlock. I'm curious if he was inspired by Sherlock.\n",
            "--------------------\n",
            "Fun, fast paced, and disturbing tale of murder. Great beach read while in France, which is where its set. \n",
            " I enjoyed learning more about perfume and the power of scent - hearing the descriptions of how different kinds of scents are captured (distilled, oils, etc) was very interesting. I think in general we (certainly I) don't pay much attention to the power that scent holds over us in different situations. I will pay more attention now after reading this. Though I wish I had a stronger nose - curious if there are ways to cultivate that. My wife and I once bought a game at a wine store that had 30 different materials that they use in wine and you had to guess what they were. I think I got 3 out of 30 right :( \n",
            " This was a dark and twisted book though be forewarned - I generally don't like reading first person accounts of murderers - but in this case it was gripping. And the climax (yes that was a pun) was so ridiculous and fun that it was worth it.\n",
            "--------------------\n",
            "A fun book that gives you a sense of living in Paris as an expat and what to appreciate about French culture. Narrated by the author so definitely recommend listening. Great read while on vacation in France. I loved many of the annecdotes were hilarious - eg the one about how the gym had no plan for visiting every day they only had a once a week plan. Or the one comparing the French fax error codes to French culture.\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRf8gxJmmqSZ",
        "outputId": "0bfa9528-7d19-4c7c-ecad-7c31c09a185f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.3)\n",
            "Requirement already satisfied: idna<2.6,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.5)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO5R3HEAtzzK"
      },
      "source": [
        "import nltk\n",
        "import gensim\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anMn9lqFHVI7",
        "outputId": "327dfedb-c3f6-4f1f-eb79-14b8db95a54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# each review is a document\n",
        "file_docs = []\n",
        "\n",
        "# tokenize sentences\n",
        "for review_text in goodreads_review_texts:\n",
        "  print(type(review_text))\n",
        "  line = sent_tokenize(review_text)\n",
        "  file_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs))\n",
        "file_docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "Number of documents: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Mind blowingly cool.',\n",
              "  \"Best science fiction I've read in some time.\",\n",
              "  'I just loved all the descriptions of the society of the future - how they lived in trees, the notion of owning property or even getting married was gone.',\n",
              "  'How every surface was a screen.',\n",
              "  'The undulations of how society responds to the Trisolaran threat seem surprising to me.',\n",
              "  \"Maybe its more the Chinese perspective, but I wouldn't have thought the ETO would exist in book 1, and I wouldn't have thought people would get so over-confident in our primitive fleet's chances given you have to think that with superior science they would have weapons - and defenses - that would just be as rifles to arrows once were.\",\n",
              "  'But the moment when Luo Ji won as a wallfacer was just too cool.',\n",
              "  'I may have actually done a fist pump.',\n",
              "  \"Though by the way, if the Dark Forest theory is right - and I see no reason why it wouldn't be - we as a society should probably stop broadcasting so much signal out into the universe.\"],\n",
              " ['This is a special book.',\n",
              "  'It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind.',\n",
              "  'This is what I love about good science fiction - it pushes your thinking about where things can go.',\n",
              "  \"It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I've read.\",\n",
              "  'For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc.',\n",
              "  'It is a book about science, and aliens.',\n",
              "  'The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell.',\n",
              "  'Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though.',\n",
              "  'But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned?',\n",
              "  'That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together.',\n",
              "  'I did like how the book unveiled the Trisolaran culture through the game.',\n",
              "  \"It was a smart way to build empathy with them and also understand what they've gone through across so many centuries.\",\n",
              "  'And who know a 3 body problem was an unsolvable math problem?',\n",
              "  \"But I still don't get who made the game - maybe that will come in the next book.\",\n",
              "  'I loved this quote: \\n \"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists?',\n",
              "  'How many neutrons and electrons?',\n",
              "  'Probably no fewer than a hundred million.',\n",
              "  'Every collision was probably the end of the civilizations and intelligences in a microcosmos.',\n",
              "  'In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons.',\n",
              "  'Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"'],\n",
              " [\"I haven't read a fun mystery book in a while and not sure I've ever read Poirot.\",\n",
              "  \"Was looking for a fun read set in France while I was on holiday there and this didn't disappoint!\",\n",
              "  'Fast paced and good mystery.',\n",
              "  'One that struck me was how similar Poirot is to Sherlock.',\n",
              "  'They are both detectives, have a ex-military sidekick who is telling the story, and solve mysteries using their superior wit.',\n",
              "  'Poirot seems like a French Sherlock.',\n",
              "  \"I'm curious if he was inspired by Sherlock.\"],\n",
              " ['Fun, fast paced, and disturbing tale of murder.',\n",
              "  'Great beach read while in France, which is where its set.',\n",
              "  'I enjoyed learning more about perfume and the power of scent - hearing the descriptions of how different kinds of scents are captured (distilled, oils, etc) was very interesting.',\n",
              "  \"I think in general we (certainly I) don't pay much attention to the power that scent holds over us in different situations.\",\n",
              "  'I will pay more attention now after reading this.',\n",
              "  'Though I wish I had a stronger nose - curious if there are ways to cultivate that.',\n",
              "  'My wife and I once bought a game at a wine store that had 30 different materials that they use in wine and you had to guess what they were.',\n",
              "  \"I think I got 3 out of 30 right :( \\n This was a dark and twisted book though be forewarned - I generally don't like reading first person accounts of murderers - but in this case it was gripping.\",\n",
              "  'And the climax (yes that was a pun) was so ridiculous and fun that it was worth it.'],\n",
              " ['A fun book that gives you a sense of living in Paris as an expat and what to appreciate about French culture.',\n",
              "  'Narrated by the author so definitely recommend listening.',\n",
              "  'Great read while on vacation in France.',\n",
              "  'I loved many of the annecdotes were hilarious - eg the one about how the gym had no plan for visiting every day they only had a once a week plan.',\n",
              "  'Or the one comparing the French fax error codes to French culture.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cahVEDtMLrco",
        "outputId": "e386901f-f64e-47c7-ab91-2647297a5a6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tokenize words and create dictionary\n",
        "\n",
        "gen_docs = []\n",
        "for review in file_docs:\n",
        "  words_in_review = []\n",
        "  for sent in review:\n",
        "    for w in word_tokenize(sent):\n",
        "      words_in_review.append(w.lower())\n",
        "  gen_docs.append(words_in_review)\n",
        "\n",
        "\n",
        "for doc in gen_docs:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mind', 'blowingly', 'cool', '.', 'best', 'science', 'fiction', 'i', \"'ve\", 'read', 'in', 'some', 'time', '.', 'i', 'just', 'loved', 'all', 'the', 'descriptions', 'of', 'the', 'society', 'of', 'the', 'future', '-', 'how', 'they', 'lived', 'in', 'trees', ',', 'the', 'notion', 'of', 'owning', 'property', 'or', 'even', 'getting', 'married', 'was', 'gone', '.', 'how', 'every', 'surface', 'was', 'a', 'screen', '.', 'the', 'undulations', 'of', 'how', 'society', 'responds', 'to', 'the', 'trisolaran', 'threat', 'seem', 'surprising', 'to', 'me', '.', 'maybe', 'its', 'more', 'the', 'chinese', 'perspective', ',', 'but', 'i', 'would', \"n't\", 'have', 'thought', 'the', 'eto', 'would', 'exist', 'in', 'book', '1', ',', 'and', 'i', 'would', \"n't\", 'have', 'thought', 'people', 'would', 'get', 'so', 'over-confident', 'in', 'our', 'primitive', 'fleet', \"'s\", 'chances', 'given', 'you', 'have', 'to', 'think', 'that', 'with', 'superior', 'science', 'they', 'would', 'have', 'weapons', '-', 'and', 'defenses', '-', 'that', 'would', 'just', 'be', 'as', 'rifles', 'to', 'arrows', 'once', 'were', '.', 'but', 'the', 'moment', 'when', 'luo', 'ji', 'won', 'as', 'a', 'wallfacer', 'was', 'just', 'too', 'cool', '.', 'i', 'may', 'have', 'actually', 'done', 'a', 'fist', 'pump', '.', 'though', 'by', 'the', 'way', ',', 'if', 'the', 'dark', 'forest', 'theory', 'is', 'right', '-', 'and', 'i', 'see', 'no', 'reason', 'why', 'it', 'would', \"n't\", 'be', '-', 'we', 'as', 'a', 'society', 'should', 'probably', 'stop', 'broadcasting', 'so', 'much', 'signal', 'out', 'into', 'the', 'universe', '.']\n",
            "['this', 'is', 'a', 'special', 'book', '.', 'it', 'started', 'slow', 'for', 'about', 'the', 'first', 'third', ',', 'then', 'in', 'the', 'middle', 'third', 'it', 'started', 'to', 'get', 'interesting', ',', 'then', 'the', 'last', 'third', 'blew', 'my', 'mind', '.', 'this', 'is', 'what', 'i', 'love', 'about', 'good', 'science', 'fiction', '-', 'it', 'pushes', 'your', 'thinking', 'about', 'where', 'things', 'can', 'go', '.', 'it', 'is', 'a', '2015', 'hugo', 'winner', ',', 'and', 'translated', 'from', 'its', 'original', 'chinese', ',', 'which', 'made', 'it', 'interesting', 'in', 'just', 'a', 'different', 'way', 'from', 'most', 'things', 'i', \"'ve\", 'read', '.', 'for', 'instance', 'the', 'intermixing', 'of', 'chinese', 'revolutionary', 'history', '-', 'how', 'they', 'kept', 'accusing', 'people', 'of', 'being', '``', 'reactionaries', \"''\", ',', 'etc', '.', 'it', 'is', 'a', 'book', 'about', 'science', ',', 'and', 'aliens', '.', 'the', 'science', 'described', 'in', 'the', 'book', 'is', 'impressive', '-', 'its', 'a', 'book', 'grounded', 'in', 'physics', 'and', 'pretty', 'accurate', 'as', 'far', 'as', 'i', 'could', 'tell', '.', 'though', 'when', 'it', 'got', 'to', 'folding', 'protons', 'into', '8', 'dimensions', 'i', 'think', 'he', 'was', 'just', 'making', 'stuff', 'up', '-', 'interesting', 'to', 'think', 'about', 'though', '.', 'but', 'what', 'would', 'happen', 'if', 'our', 'seti', 'stations', 'received', 'a', 'message', '-', 'if', 'we', 'found', 'someone', 'was', 'out', 'there', '-', 'and', 'the', 'person', 'monitoring', 'and', 'answering', 'the', 'signal', 'on', 'our', 'side', 'was', 'disillusioned', '?', 'that', 'part', 'of', 'the', 'book', 'was', 'a', 'bit', 'dark', '-', 'i', 'would', 'like', 'to', 'think', 'human', 'reaction', 'to', 'discovering', 'alien', 'civilization', 'that', 'is', 'hostile', 'would', 'be', 'more', 'like', 'enders', 'game', 'where', 'we', 'would', 'band', 'together', '.', 'i', 'did', 'like', 'how', 'the', 'book', 'unveiled', 'the', 'trisolaran', 'culture', 'through', 'the', 'game', '.', 'it', 'was', 'a', 'smart', 'way', 'to', 'build', 'empathy', 'with', 'them', 'and', 'also', 'understand', 'what', 'they', \"'ve\", 'gone', 'through', 'across', 'so', 'many', 'centuries', '.', 'and', 'who', 'know', 'a', '3', 'body', 'problem', 'was', 'an', 'unsolvable', 'math', 'problem', '?', 'but', 'i', 'still', 'do', \"n't\", 'get', 'who', 'made', 'the', 'game', '-', 'maybe', 'that', 'will', 'come', 'in', 'the', 'next', 'book', '.', 'i', 'loved', 'this', 'quote', ':', '``', 'in', 'the', 'long', 'history', 'of', 'scientific', 'progress', ',', 'how', 'many', 'protons', 'have', 'been', 'smashed', 'apart', 'in', 'accelerators', 'by', 'physicists', '?', 'how', 'many', 'neutrons', 'and', 'electrons', '?', 'probably', 'no', 'fewer', 'than', 'a', 'hundred', 'million', '.', 'every', 'collision', 'was', 'probably', 'the', 'end', 'of', 'the', 'civilizations', 'and', 'intelligences', 'in', 'a', 'microcosmos', '.', 'in', 'fact', ',', 'even', 'in', 'nature', ',', 'the', 'destruction', 'of', 'universes', 'must', 'be', 'happening', 'at', 'every', 'second', '--', 'for', 'example', ',', 'through', 'the', 'decay', 'of', 'neutrons', '.', 'also', ',', 'a', 'high-energy', 'cosmic', 'ray', 'entering', 'the', 'atmosphere', 'may', 'destroy', 'thousands', 'of', 'such', 'miniature', 'universes', '...', '.', \"''\"]\n",
            "['i', 'have', \"n't\", 'read', 'a', 'fun', 'mystery', 'book', 'in', 'a', 'while', 'and', 'not', 'sure', 'i', \"'ve\", 'ever', 'read', 'poirot', '.', 'was', 'looking', 'for', 'a', 'fun', 'read', 'set', 'in', 'france', 'while', 'i', 'was', 'on', 'holiday', 'there', 'and', 'this', 'did', \"n't\", 'disappoint', '!', 'fast', 'paced', 'and', 'good', 'mystery', '.', 'one', 'that', 'struck', 'me', 'was', 'how', 'similar', 'poirot', 'is', 'to', 'sherlock', '.', 'they', 'are', 'both', 'detectives', ',', 'have', 'a', 'ex-military', 'sidekick', 'who', 'is', 'telling', 'the', 'story', ',', 'and', 'solve', 'mysteries', 'using', 'their', 'superior', 'wit', '.', 'poirot', 'seems', 'like', 'a', 'french', 'sherlock', '.', 'i', \"'m\", 'curious', 'if', 'he', 'was', 'inspired', 'by', 'sherlock', '.']\n",
            "['fun', ',', 'fast', 'paced', ',', 'and', 'disturbing', 'tale', 'of', 'murder', '.', 'great', 'beach', 'read', 'while', 'in', 'france', ',', 'which', 'is', 'where', 'its', 'set', '.', 'i', 'enjoyed', 'learning', 'more', 'about', 'perfume', 'and', 'the', 'power', 'of', 'scent', '-', 'hearing', 'the', 'descriptions', 'of', 'how', 'different', 'kinds', 'of', 'scents', 'are', 'captured', '(', 'distilled', ',', 'oils', ',', 'etc', ')', 'was', 'very', 'interesting', '.', 'i', 'think', 'in', 'general', 'we', '(', 'certainly', 'i', ')', 'do', \"n't\", 'pay', 'much', 'attention', 'to', 'the', 'power', 'that', 'scent', 'holds', 'over', 'us', 'in', 'different', 'situations', '.', 'i', 'will', 'pay', 'more', 'attention', 'now', 'after', 'reading', 'this', '.', 'though', 'i', 'wish', 'i', 'had', 'a', 'stronger', 'nose', '-', 'curious', 'if', 'there', 'are', 'ways', 'to', 'cultivate', 'that', '.', 'my', 'wife', 'and', 'i', 'once', 'bought', 'a', 'game', 'at', 'a', 'wine', 'store', 'that', 'had', '30', 'different', 'materials', 'that', 'they', 'use', 'in', 'wine', 'and', 'you', 'had', 'to', 'guess', 'what', 'they', 'were', '.', 'i', 'think', 'i', 'got', '3', 'out', 'of', '30', 'right', ':', '(', 'this', 'was', 'a', 'dark', 'and', 'twisted', 'book', 'though', 'be', 'forewarned', '-', 'i', 'generally', 'do', \"n't\", 'like', 'reading', 'first', 'person', 'accounts', 'of', 'murderers', '-', 'but', 'in', 'this', 'case', 'it', 'was', 'gripping', '.', 'and', 'the', 'climax', '(', 'yes', 'that', 'was', 'a', 'pun', ')', 'was', 'so', 'ridiculous', 'and', 'fun', 'that', 'it', 'was', 'worth', 'it', '.']\n",
            "['a', 'fun', 'book', 'that', 'gives', 'you', 'a', 'sense', 'of', 'living', 'in', 'paris', 'as', 'an', 'expat', 'and', 'what', 'to', 'appreciate', 'about', 'french', 'culture', '.', 'narrated', 'by', 'the', 'author', 'so', 'definitely', 'recommend', 'listening', '.', 'great', 'read', 'while', 'on', 'vacation', 'in', 'france', '.', 'i', 'loved', 'many', 'of', 'the', 'annecdotes', 'were', 'hilarious', '-', 'eg', 'the', 'one', 'about', 'how', 'the', 'gym', 'had', 'no', 'plan', 'for', 'visiting', 'every', 'day', 'they', 'only', 'had', 'a', 'once', 'a', 'week', 'plan', '.', 'or', 'the', 'one', 'comparing', 'the', 'french', 'fax', 'error', 'codes', 'to', 'french', 'culture', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc9xgqdsVgNF",
        "outputId": "668ae172-0a4e-479e-c716-a55dd43ec768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a Dictionary object that maps each word to a unique id\n",
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "print(dictionary.token2id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"'s\": 0, \"'ve\": 1, ',': 2, '-': 3, '.': 4, '1': 5, 'a': 6, 'actually': 7, 'all': 8, 'and': 9, 'arrows': 10, 'as': 11, 'be': 12, 'best': 13, 'blowingly': 14, 'book': 15, 'broadcasting': 16, 'but': 17, 'by': 18, 'chances': 19, 'chinese': 20, 'cool': 21, 'dark': 22, 'defenses': 23, 'descriptions': 24, 'done': 25, 'eto': 26, 'even': 27, 'every': 28, 'exist': 29, 'fiction': 30, 'fist': 31, 'fleet': 32, 'forest': 33, 'future': 34, 'get': 35, 'getting': 36, 'given': 37, 'gone': 38, 'have': 39, 'how': 40, 'i': 41, 'if': 42, 'in': 43, 'into': 44, 'is': 45, 'it': 46, 'its': 47, 'ji': 48, 'just': 49, 'lived': 50, 'loved': 51, 'luo': 52, 'married': 53, 'may': 54, 'maybe': 55, 'me': 56, 'mind': 57, 'moment': 58, 'more': 59, 'much': 60, \"n't\": 61, 'no': 62, 'notion': 63, 'of': 64, 'once': 65, 'or': 66, 'our': 67, 'out': 68, 'over-confident': 69, 'owning': 70, 'people': 71, 'perspective': 72, 'primitive': 73, 'probably': 74, 'property': 75, 'pump': 76, 'read': 77, 'reason': 78, 'responds': 79, 'rifles': 80, 'right': 81, 'science': 82, 'screen': 83, 'see': 84, 'seem': 85, 'should': 86, 'signal': 87, 'so': 88, 'society': 89, 'some': 90, 'stop': 91, 'superior': 92, 'surface': 93, 'surprising': 94, 'that': 95, 'the': 96, 'theory': 97, 'they': 98, 'think': 99, 'though': 100, 'thought': 101, 'threat': 102, 'time': 103, 'to': 104, 'too': 105, 'trees': 106, 'trisolaran': 107, 'undulations': 108, 'universe': 109, 'wallfacer': 110, 'was': 111, 'way': 112, 'we': 113, 'weapons': 114, 'were': 115, 'when': 116, 'why': 117, 'with': 118, 'won': 119, 'would': 120, 'you': 121, \"''\": 122, '--': 123, '...': 124, '2015': 125, '3': 126, '8': 127, ':': 128, '?': 129, '``': 130, 'about': 131, 'accelerators': 132, 'accurate': 133, 'accusing': 134, 'across': 135, 'alien': 136, 'aliens': 137, 'also': 138, 'an': 139, 'answering': 140, 'apart': 141, 'at': 142, 'atmosphere': 143, 'band': 144, 'been': 145, 'being': 146, 'bit': 147, 'blew': 148, 'body': 149, 'build': 150, 'can': 151, 'centuries': 152, 'civilization': 153, 'civilizations': 154, 'collision': 155, 'come': 156, 'cosmic': 157, 'could': 158, 'culture': 159, 'decay': 160, 'described': 161, 'destroy': 162, 'destruction': 163, 'did': 164, 'different': 165, 'dimensions': 166, 'discovering': 167, 'disillusioned': 168, 'do': 169, 'electrons': 170, 'empathy': 171, 'end': 172, 'enders': 173, 'entering': 174, 'etc': 175, 'example': 176, 'fact': 177, 'far': 178, 'fewer': 179, 'first': 180, 'folding': 181, 'for': 182, 'found': 183, 'from': 184, 'game': 185, 'go': 186, 'good': 187, 'got': 188, 'grounded': 189, 'happen': 190, 'happening': 191, 'he': 192, 'high-energy': 193, 'history': 194, 'hostile': 195, 'hugo': 196, 'human': 197, 'hundred': 198, 'impressive': 199, 'instance': 200, 'intelligences': 201, 'interesting': 202, 'intermixing': 203, 'kept': 204, 'know': 205, 'last': 206, 'like': 207, 'long': 208, 'love': 209, 'made': 210, 'making': 211, 'many': 212, 'math': 213, 'message': 214, 'microcosmos': 215, 'middle': 216, 'million': 217, 'miniature': 218, 'monitoring': 219, 'most': 220, 'must': 221, 'my': 222, 'nature': 223, 'neutrons': 224, 'next': 225, 'on': 226, 'original': 227, 'part': 228, 'person': 229, 'physicists': 230, 'physics': 231, 'pretty': 232, 'problem': 233, 'progress': 234, 'protons': 235, 'pushes': 236, 'quote': 237, 'ray': 238, 'reaction': 239, 'reactionaries': 240, 'received': 241, 'revolutionary': 242, 'scientific': 243, 'second': 244, 'seti': 245, 'side': 246, 'slow': 247, 'smart': 248, 'smashed': 249, 'someone': 250, 'special': 251, 'started': 252, 'stations': 253, 'still': 254, 'stuff': 255, 'such': 256, 'tell': 257, 'than': 258, 'them': 259, 'then': 260, 'there': 261, 'things': 262, 'thinking': 263, 'third': 264, 'this': 265, 'thousands': 266, 'through': 267, 'together': 268, 'translated': 269, 'understand': 270, 'universes': 271, 'unsolvable': 272, 'unveiled': 273, 'up': 274, 'what': 275, 'where': 276, 'which': 277, 'who': 278, 'will': 279, 'winner': 280, 'your': 281, '!': 282, \"'m\": 283, 'are': 284, 'both': 285, 'curious': 286, 'detectives': 287, 'disappoint': 288, 'ever': 289, 'ex-military': 290, 'fast': 291, 'france': 292, 'french': 293, 'fun': 294, 'holiday': 295, 'inspired': 296, 'looking': 297, 'mysteries': 298, 'mystery': 299, 'not': 300, 'one': 301, 'paced': 302, 'poirot': 303, 'seems': 304, 'set': 305, 'sherlock': 306, 'sidekick': 307, 'similar': 308, 'solve': 309, 'story': 310, 'struck': 311, 'sure': 312, 'telling': 313, 'their': 314, 'using': 315, 'while': 316, 'wit': 317, '(': 318, ')': 319, '30': 320, 'accounts': 321, 'after': 322, 'attention': 323, 'beach': 324, 'bought': 325, 'captured': 326, 'case': 327, 'certainly': 328, 'climax': 329, 'cultivate': 330, 'distilled': 331, 'disturbing': 332, 'enjoyed': 333, 'forewarned': 334, 'general': 335, 'generally': 336, 'great': 337, 'gripping': 338, 'guess': 339, 'had': 340, 'hearing': 341, 'holds': 342, 'kinds': 343, 'learning': 344, 'materials': 345, 'murder': 346, 'murderers': 347, 'nose': 348, 'now': 349, 'oils': 350, 'over': 351, 'pay': 352, 'perfume': 353, 'power': 354, 'pun': 355, 'reading': 356, 'ridiculous': 357, 'scent': 358, 'scents': 359, 'situations': 360, 'store': 361, 'stronger': 362, 'tale': 363, 'twisted': 364, 'us': 365, 'use': 366, 'very': 367, 'ways': 368, 'wife': 369, 'wine': 370, 'wish': 371, 'worth': 372, 'yes': 373, 'annecdotes': 374, 'appreciate': 375, 'author': 376, 'codes': 377, 'comparing': 378, 'day': 379, 'definitely': 380, 'eg': 381, 'error': 382, 'expat': 383, 'fax': 384, 'gives': 385, 'gym': 386, 'hilarious': 387, 'listening': 388, 'living': 389, 'narrated': 390, 'only': 391, 'paris': 392, 'plan': 393, 'recommend': 394, 'sense': 395, 'vacation': 396, 'visiting': 397, 'week': 398}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-Y9yFZ2V7CX",
        "outputId": "d73780d2-55cc-4251-919a-08e5ec97d49f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a bag of words\n",
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1),\n",
              "  (1, 1),\n",
              "  (2, 4),\n",
              "  (3, 5),\n",
              "  (4, 9),\n",
              "  (5, 1),\n",
              "  (6, 4),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 3),\n",
              "  (10, 1),\n",
              "  (11, 3),\n",
              "  (12, 2),\n",
              "  (13, 1),\n",
              "  (14, 1),\n",
              "  (15, 1),\n",
              "  (16, 1),\n",
              "  (17, 2),\n",
              "  (18, 1),\n",
              "  (19, 1),\n",
              "  (20, 1),\n",
              "  (21, 2),\n",
              "  (22, 1),\n",
              "  (23, 1),\n",
              "  (24, 1),\n",
              "  (25, 1),\n",
              "  (26, 1),\n",
              "  (27, 1),\n",
              "  (28, 1),\n",
              "  (29, 1),\n",
              "  (30, 1),\n",
              "  (31, 1),\n",
              "  (32, 1),\n",
              "  (33, 1),\n",
              "  (34, 1),\n",
              "  (35, 1),\n",
              "  (36, 1),\n",
              "  (37, 1),\n",
              "  (38, 1),\n",
              "  (39, 5),\n",
              "  (40, 3),\n",
              "  (41, 6),\n",
              "  (42, 1),\n",
              "  (43, 4),\n",
              "  (44, 1),\n",
              "  (45, 1),\n",
              "  (46, 1),\n",
              "  (47, 1),\n",
              "  (48, 1),\n",
              "  (49, 3),\n",
              "  (50, 1),\n",
              "  (51, 1),\n",
              "  (52, 1),\n",
              "  (53, 1),\n",
              "  (54, 1),\n",
              "  (55, 1),\n",
              "  (56, 1),\n",
              "  (57, 1),\n",
              "  (58, 1),\n",
              "  (59, 1),\n",
              "  (60, 1),\n",
              "  (61, 3),\n",
              "  (62, 1),\n",
              "  (63, 1),\n",
              "  (64, 4),\n",
              "  (65, 1),\n",
              "  (66, 1),\n",
              "  (67, 1),\n",
              "  (68, 1),\n",
              "  (69, 1),\n",
              "  (70, 1),\n",
              "  (71, 1),\n",
              "  (72, 1),\n",
              "  (73, 1),\n",
              "  (74, 1),\n",
              "  (75, 1),\n",
              "  (76, 1),\n",
              "  (77, 1),\n",
              "  (78, 1),\n",
              "  (79, 1),\n",
              "  (80, 1),\n",
              "  (81, 1),\n",
              "  (82, 2),\n",
              "  (83, 1),\n",
              "  (84, 1),\n",
              "  (85, 1),\n",
              "  (86, 1),\n",
              "  (87, 1),\n",
              "  (88, 2),\n",
              "  (89, 3),\n",
              "  (90, 1),\n",
              "  (91, 1),\n",
              "  (92, 1),\n",
              "  (93, 1),\n",
              "  (94, 1),\n",
              "  (95, 2),\n",
              "  (96, 12),\n",
              "  (97, 1),\n",
              "  (98, 2),\n",
              "  (99, 1),\n",
              "  (100, 1),\n",
              "  (101, 2),\n",
              "  (102, 1),\n",
              "  (103, 1),\n",
              "  (104, 4),\n",
              "  (105, 1),\n",
              "  (106, 1),\n",
              "  (107, 1),\n",
              "  (108, 1),\n",
              "  (109, 1),\n",
              "  (110, 1),\n",
              "  (111, 3),\n",
              "  (112, 1),\n",
              "  (113, 1),\n",
              "  (114, 1),\n",
              "  (115, 1),\n",
              "  (116, 1),\n",
              "  (117, 1),\n",
              "  (118, 1),\n",
              "  (119, 1),\n",
              "  (120, 7),\n",
              "  (121, 1)],\n",
              " [(1, 2),\n",
              "  (2, 11),\n",
              "  (3, 8),\n",
              "  (4, 16),\n",
              "  (6, 12),\n",
              "  (9, 9),\n",
              "  (11, 2),\n",
              "  (12, 2),\n",
              "  (15, 7),\n",
              "  (17, 2),\n",
              "  (18, 1),\n",
              "  (20, 2),\n",
              "  (22, 1),\n",
              "  (27, 1),\n",
              "  (28, 2),\n",
              "  (30, 1),\n",
              "  (35, 2),\n",
              "  (38, 1),\n",
              "  (39, 1),\n",
              "  (40, 4),\n",
              "  (41, 8),\n",
              "  (42, 2),\n",
              "  (43, 10),\n",
              "  (44, 1),\n",
              "  (45, 6),\n",
              "  (46, 8),\n",
              "  (47, 2),\n",
              "  (49, 2),\n",
              "  (51, 1),\n",
              "  (54, 1),\n",
              "  (55, 1),\n",
              "  (57, 1),\n",
              "  (59, 1),\n",
              "  (61, 1),\n",
              "  (62, 1),\n",
              "  (64, 8),\n",
              "  (67, 2),\n",
              "  (68, 1),\n",
              "  (71, 1),\n",
              "  (74, 2),\n",
              "  (77, 1),\n",
              "  (82, 3),\n",
              "  (87, 1),\n",
              "  (88, 1),\n",
              "  (95, 3),\n",
              "  (96, 20),\n",
              "  (98, 2),\n",
              "  (99, 3),\n",
              "  (100, 2),\n",
              "  (104, 6),\n",
              "  (107, 1),\n",
              "  (111, 7),\n",
              "  (112, 2),\n",
              "  (113, 2),\n",
              "  (116, 1),\n",
              "  (118, 1),\n",
              "  (120, 4),\n",
              "  (122, 2),\n",
              "  (123, 1),\n",
              "  (124, 1),\n",
              "  (125, 1),\n",
              "  (126, 1),\n",
              "  (127, 1),\n",
              "  (128, 1),\n",
              "  (129, 4),\n",
              "  (130, 2),\n",
              "  (131, 5),\n",
              "  (132, 1),\n",
              "  (133, 1),\n",
              "  (134, 1),\n",
              "  (135, 1),\n",
              "  (136, 1),\n",
              "  (137, 1),\n",
              "  (138, 2),\n",
              "  (139, 1),\n",
              "  (140, 1),\n",
              "  (141, 1),\n",
              "  (142, 1),\n",
              "  (143, 1),\n",
              "  (144, 1),\n",
              "  (145, 1),\n",
              "  (146, 1),\n",
              "  (147, 1),\n",
              "  (148, 1),\n",
              "  (149, 1),\n",
              "  (150, 1),\n",
              "  (151, 1),\n",
              "  (152, 1),\n",
              "  (153, 1),\n",
              "  (154, 1),\n",
              "  (155, 1),\n",
              "  (156, 1),\n",
              "  (157, 1),\n",
              "  (158, 1),\n",
              "  (159, 1),\n",
              "  (160, 1),\n",
              "  (161, 1),\n",
              "  (162, 1),\n",
              "  (163, 1),\n",
              "  (164, 1),\n",
              "  (165, 1),\n",
              "  (166, 1),\n",
              "  (167, 1),\n",
              "  (168, 1),\n",
              "  (169, 1),\n",
              "  (170, 1),\n",
              "  (171, 1),\n",
              "  (172, 1),\n",
              "  (173, 1),\n",
              "  (174, 1),\n",
              "  (175, 1),\n",
              "  (176, 1),\n",
              "  (177, 1),\n",
              "  (178, 1),\n",
              "  (179, 1),\n",
              "  (180, 1),\n",
              "  (181, 1),\n",
              "  (182, 3),\n",
              "  (183, 1),\n",
              "  (184, 2),\n",
              "  (185, 3),\n",
              "  (186, 1),\n",
              "  (187, 1),\n",
              "  (188, 1),\n",
              "  (189, 1),\n",
              "  (190, 1),\n",
              "  (191, 1),\n",
              "  (192, 1),\n",
              "  (193, 1),\n",
              "  (194, 2),\n",
              "  (195, 1),\n",
              "  (196, 1),\n",
              "  (197, 1),\n",
              "  (198, 1),\n",
              "  (199, 1),\n",
              "  (200, 1),\n",
              "  (201, 1),\n",
              "  (202, 3),\n",
              "  (203, 1),\n",
              "  (204, 1),\n",
              "  (205, 1),\n",
              "  (206, 1),\n",
              "  (207, 3),\n",
              "  (208, 1),\n",
              "  (209, 1),\n",
              "  (210, 2),\n",
              "  (211, 1),\n",
              "  (212, 3),\n",
              "  (213, 1),\n",
              "  (214, 1),\n",
              "  (215, 1),\n",
              "  (216, 1),\n",
              "  (217, 1),\n",
              "  (218, 1),\n",
              "  (219, 1),\n",
              "  (220, 1),\n",
              "  (221, 1),\n",
              "  (222, 1),\n",
              "  (223, 1),\n",
              "  (224, 2),\n",
              "  (225, 1),\n",
              "  (226, 1),\n",
              "  (227, 1),\n",
              "  (228, 1),\n",
              "  (229, 1),\n",
              "  (230, 1),\n",
              "  (231, 1),\n",
              "  (232, 1),\n",
              "  (233, 2),\n",
              "  (234, 1),\n",
              "  (235, 2),\n",
              "  (236, 1),\n",
              "  (237, 1),\n",
              "  (238, 1),\n",
              "  (239, 1),\n",
              "  (240, 1),\n",
              "  (241, 1),\n",
              "  (242, 1),\n",
              "  (243, 1),\n",
              "  (244, 1),\n",
              "  (245, 1),\n",
              "  (246, 1),\n",
              "  (247, 1),\n",
              "  (248, 1),\n",
              "  (249, 1),\n",
              "  (250, 1),\n",
              "  (251, 1),\n",
              "  (252, 2),\n",
              "  (253, 1),\n",
              "  (254, 1),\n",
              "  (255, 1),\n",
              "  (256, 1),\n",
              "  (257, 1),\n",
              "  (258, 1),\n",
              "  (259, 1),\n",
              "  (260, 2),\n",
              "  (261, 1),\n",
              "  (262, 2),\n",
              "  (263, 1),\n",
              "  (264, 3),\n",
              "  (265, 3),\n",
              "  (266, 1),\n",
              "  (267, 3),\n",
              "  (268, 1),\n",
              "  (269, 1),\n",
              "  (270, 1),\n",
              "  (271, 2),\n",
              "  (272, 1),\n",
              "  (273, 1),\n",
              "  (274, 1),\n",
              "  (275, 3),\n",
              "  (276, 2),\n",
              "  (277, 1),\n",
              "  (278, 2),\n",
              "  (279, 1),\n",
              "  (280, 1),\n",
              "  (281, 1)],\n",
              " [(1, 1),\n",
              "  (2, 2),\n",
              "  (4, 6),\n",
              "  (6, 5),\n",
              "  (9, 4),\n",
              "  (15, 1),\n",
              "  (18, 1),\n",
              "  (39, 2),\n",
              "  (40, 1),\n",
              "  (41, 4),\n",
              "  (42, 1),\n",
              "  (43, 2),\n",
              "  (45, 2),\n",
              "  (56, 1),\n",
              "  (61, 2),\n",
              "  (77, 3),\n",
              "  (92, 1),\n",
              "  (95, 1),\n",
              "  (96, 1),\n",
              "  (98, 1),\n",
              "  (104, 1),\n",
              "  (111, 4),\n",
              "  (164, 1),\n",
              "  (182, 1),\n",
              "  (187, 1),\n",
              "  (192, 1),\n",
              "  (207, 1),\n",
              "  (226, 1),\n",
              "  (261, 1),\n",
              "  (265, 1),\n",
              "  (278, 1),\n",
              "  (282, 1),\n",
              "  (283, 1),\n",
              "  (284, 1),\n",
              "  (285, 1),\n",
              "  (286, 1),\n",
              "  (287, 1),\n",
              "  (288, 1),\n",
              "  (289, 1),\n",
              "  (290, 1),\n",
              "  (291, 1),\n",
              "  (292, 1),\n",
              "  (293, 1),\n",
              "  (294, 2),\n",
              "  (295, 1),\n",
              "  (296, 1),\n",
              "  (297, 1),\n",
              "  (298, 1),\n",
              "  (299, 2),\n",
              "  (300, 1),\n",
              "  (301, 1),\n",
              "  (302, 1),\n",
              "  (303, 3),\n",
              "  (304, 1),\n",
              "  (305, 1),\n",
              "  (306, 3),\n",
              "  (307, 1),\n",
              "  (308, 1),\n",
              "  (309, 1),\n",
              "  (310, 1),\n",
              "  (311, 1),\n",
              "  (312, 1),\n",
              "  (313, 1),\n",
              "  (314, 1),\n",
              "  (315, 1),\n",
              "  (316, 2),\n",
              "  (317, 1)],\n",
              " [(2, 5),\n",
              "  (3, 4),\n",
              "  (4, 9),\n",
              "  (6, 5),\n",
              "  (9, 7),\n",
              "  (12, 1),\n",
              "  (15, 1),\n",
              "  (17, 1),\n",
              "  (22, 1),\n",
              "  (24, 1),\n",
              "  (40, 1),\n",
              "  (41, 10),\n",
              "  (42, 1),\n",
              "  (43, 5),\n",
              "  (45, 1),\n",
              "  (46, 3),\n",
              "  (47, 1),\n",
              "  (59, 2),\n",
              "  (60, 1),\n",
              "  (61, 2),\n",
              "  (64, 6),\n",
              "  (65, 1),\n",
              "  (68, 1),\n",
              "  (77, 1),\n",
              "  (81, 1),\n",
              "  (88, 1),\n",
              "  (95, 6),\n",
              "  (96, 4),\n",
              "  (98, 2),\n",
              "  (99, 2),\n",
              "  (100, 2),\n",
              "  (104, 3),\n",
              "  (111, 6),\n",
              "  (113, 1),\n",
              "  (115, 1),\n",
              "  (121, 1),\n",
              "  (126, 1),\n",
              "  (128, 1),\n",
              "  (131, 1),\n",
              "  (142, 1),\n",
              "  (165, 3),\n",
              "  (169, 2),\n",
              "  (175, 1),\n",
              "  (180, 1),\n",
              "  (185, 1),\n",
              "  (188, 1),\n",
              "  (202, 1),\n",
              "  (207, 1),\n",
              "  (222, 1),\n",
              "  (229, 1),\n",
              "  (261, 1),\n",
              "  (265, 3),\n",
              "  (275, 1),\n",
              "  (276, 1),\n",
              "  (277, 1),\n",
              "  (279, 1),\n",
              "  (284, 2),\n",
              "  (286, 1),\n",
              "  (291, 1),\n",
              "  (292, 1),\n",
              "  (294, 2),\n",
              "  (302, 1),\n",
              "  (305, 1),\n",
              "  (316, 1),\n",
              "  (318, 4),\n",
              "  (319, 3),\n",
              "  (320, 2),\n",
              "  (321, 1),\n",
              "  (322, 1),\n",
              "  (323, 2),\n",
              "  (324, 1),\n",
              "  (325, 1),\n",
              "  (326, 1),\n",
              "  (327, 1),\n",
              "  (328, 1),\n",
              "  (329, 1),\n",
              "  (330, 1),\n",
              "  (331, 1),\n",
              "  (332, 1),\n",
              "  (333, 1),\n",
              "  (334, 1),\n",
              "  (335, 1),\n",
              "  (336, 1),\n",
              "  (337, 1),\n",
              "  (338, 1),\n",
              "  (339, 1),\n",
              "  (340, 3),\n",
              "  (341, 1),\n",
              "  (342, 1),\n",
              "  (343, 1),\n",
              "  (344, 1),\n",
              "  (345, 1),\n",
              "  (346, 1),\n",
              "  (347, 1),\n",
              "  (348, 1),\n",
              "  (349, 1),\n",
              "  (350, 1),\n",
              "  (351, 1),\n",
              "  (352, 2),\n",
              "  (353, 1),\n",
              "  (354, 2),\n",
              "  (355, 1),\n",
              "  (356, 2),\n",
              "  (357, 1),\n",
              "  (358, 2),\n",
              "  (359, 1),\n",
              "  (360, 1),\n",
              "  (361, 1),\n",
              "  (362, 1),\n",
              "  (363, 1),\n",
              "  (364, 1),\n",
              "  (365, 1),\n",
              "  (366, 1),\n",
              "  (367, 1),\n",
              "  (368, 1),\n",
              "  (369, 1),\n",
              "  (370, 2),\n",
              "  (371, 1),\n",
              "  (372, 1),\n",
              "  (373, 1)],\n",
              " [(3, 1),\n",
              "  (4, 5),\n",
              "  (6, 4),\n",
              "  (9, 1),\n",
              "  (11, 1),\n",
              "  (15, 1),\n",
              "  (18, 1),\n",
              "  (28, 1),\n",
              "  (40, 1),\n",
              "  (41, 1),\n",
              "  (43, 2),\n",
              "  (51, 1),\n",
              "  (62, 1),\n",
              "  (64, 2),\n",
              "  (65, 1),\n",
              "  (66, 1),\n",
              "  (77, 1),\n",
              "  (88, 1),\n",
              "  (95, 1),\n",
              "  (96, 6),\n",
              "  (98, 1),\n",
              "  (104, 2),\n",
              "  (115, 1),\n",
              "  (121, 1),\n",
              "  (131, 2),\n",
              "  (139, 1),\n",
              "  (159, 2),\n",
              "  (182, 1),\n",
              "  (212, 1),\n",
              "  (226, 1),\n",
              "  (275, 1),\n",
              "  (292, 1),\n",
              "  (293, 3),\n",
              "  (294, 1),\n",
              "  (301, 2),\n",
              "  (316, 1),\n",
              "  (337, 1),\n",
              "  (340, 2),\n",
              "  (374, 1),\n",
              "  (375, 1),\n",
              "  (376, 1),\n",
              "  (377, 1),\n",
              "  (378, 1),\n",
              "  (379, 1),\n",
              "  (380, 1),\n",
              "  (381, 1),\n",
              "  (382, 1),\n",
              "  (383, 1),\n",
              "  (384, 1),\n",
              "  (385, 1),\n",
              "  (386, 1),\n",
              "  (387, 1),\n",
              "  (388, 1),\n",
              "  (389, 1),\n",
              "  (390, 1),\n",
              "  (391, 1),\n",
              "  (392, 1),\n",
              "  (393, 2),\n",
              "  (394, 1),\n",
              "  (395, 1),\n",
              "  (396, 1),\n",
              "  (397, 1),\n",
              "  (398, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njrALhWkWWdW",
        "outputId": "4ac47936-1e68-4fb9-b48d-30fdc77d661e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "for doc in tf_idf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"'s\", 0.1], [\"'ve\", 0.03], [',', 0.05], ['-', 0.07], ['1', 0.1], ['actually', 0.1], ['all', 0.1], ['arrows', 0.1], ['as', 0.09], ['be', 0.06], ['best', 0.1], ['blowingly', 0.1], ['broadcasting', 0.1], ['but', 0.06], ['by', 0.01], ['chances', 0.1], ['chinese', 0.06], ['cool', 0.2], ['dark', 0.03], ['defenses', 0.1], ['descriptions', 0.06], ['done', 0.1], ['eto', 0.1], ['even', 0.06], ['every', 0.03], ['exist', 0.1], ['fiction', 0.06], ['fist', 0.1], ['fleet', 0.1], ['forest', 0.1], ['future', 0.1], ['get', 0.06], ['getting', 0.1], ['given', 0.1], ['gone', 0.06], ['have', 0.15], ['if', 0.01], ['into', 0.06], ['is', 0.01], ['it', 0.03], ['its', 0.03], ['ji', 0.1], ['just', 0.17], ['lived', 0.1], ['loved', 0.03], ['luo', 0.1], ['married', 0.1], ['may', 0.06], ['maybe', 0.06], ['me', 0.06], ['mind', 0.06], ['moment', 0.1], ['more', 0.03], ['much', 0.06], [\"n't\", 0.04], ['no', 0.03], ['notion', 0.1], ['of', 0.05], ['once', 0.03], ['or', 0.06], ['our', 0.06], ['out', 0.03], ['over-confident', 0.1], ['owning', 0.1], ['people', 0.06], ['perspective', 0.1], ['primitive', 0.1], ['probably', 0.06], ['property', 0.1], ['pump', 0.1], ['reason', 0.1], ['responds', 0.1], ['rifles', 0.1], ['right', 0.06], ['science', 0.11], ['screen', 0.1], ['see', 0.1], ['seem', 0.1], ['should', 0.1], ['signal', 0.06], ['so', 0.03], ['society', 0.29], ['some', 0.1], ['stop', 0.1], ['superior', 0.06], ['surface', 0.1], ['surprising', 0.1], ['theory', 0.1], ['think', 0.03], ['though', 0.03], ['thought', 0.2], ['threat', 0.1], ['time', 0.1], ['too', 0.1], ['trees', 0.1], ['trisolaran', 0.06], ['undulations', 0.1], ['universe', 0.1], ['wallfacer', 0.1], ['was', 0.04], ['way', 0.06], ['we', 0.03], ['weapons', 0.1], ['were', 0.03], ['when', 0.06], ['why', 0.1], ['with', 0.06], ['won', 0.1], ['would', 0.39], ['you', 0.03]]\n",
            "[[\"'ve\", 0.04], [',', 0.09], ['-', 0.07], ['as', 0.04], ['be', 0.04], ['but', 0.04], ['by', 0.01], ['chinese', 0.07], ['dark', 0.02], ['even', 0.04], ['every', 0.04], ['fiction', 0.04], ['get', 0.07], ['gone', 0.04], ['have', 0.02], ['if', 0.02], ['into', 0.04], ['is', 0.05], ['it', 0.16], ['its', 0.04], ['just', 0.07], ['loved', 0.02], ['may', 0.04], ['maybe', 0.04], ['mind', 0.04], ['more', 0.02], [\"n't\", 0.01], ['no', 0.02], ['of', 0.07], ['our', 0.07], ['out', 0.02], ['people', 0.04], ['probably', 0.07], ['science', 0.11], ['signal', 0.04], ['so', 0.01], ['think', 0.06], ['though', 0.04], ['trisolaran', 0.04], ['was', 0.06], ['way', 0.07], ['we', 0.04], ['when', 0.04], ['with', 0.04], ['would', 0.14], [\"''\", 0.12], ['--', 0.06], ['...', 0.06], ['2015', 0.06], ['3', 0.04], ['8', 0.06], [':', 0.04], ['?', 0.25], ['``', 0.12], ['about', 0.1], ['accelerators', 0.06], ['accurate', 0.06], ['accusing', 0.06], ['across', 0.06], ['alien', 0.06], ['aliens', 0.06], ['also', 0.12], ['an', 0.04], ['answering', 0.06], ['apart', 0.06], ['at', 0.04], ['atmosphere', 0.06], ['band', 0.06], ['been', 0.06], ['being', 0.06], ['bit', 0.06], ['blew', 0.06], ['body', 0.06], ['build', 0.06], ['can', 0.06], ['centuries', 0.06], ['civilization', 0.06], ['civilizations', 0.06], ['collision', 0.06], ['come', 0.06], ['cosmic', 0.06], ['could', 0.06], ['culture', 0.04], ['decay', 0.06], ['described', 0.06], ['destroy', 0.06], ['destruction', 0.06], ['did', 0.04], ['different', 0.04], ['dimensions', 0.06], ['discovering', 0.06], ['disillusioned', 0.06], ['do', 0.04], ['electrons', 0.06], ['empathy', 0.06], ['end', 0.06], ['enders', 0.06], ['entering', 0.06], ['etc', 0.04], ['example', 0.06], ['fact', 0.06], ['far', 0.06], ['fewer', 0.06], ['first', 0.04], ['folding', 0.06], ['for', 0.06], ['found', 0.06], ['from', 0.12], ['game', 0.11], ['go', 0.06], ['good', 0.04], ['got', 0.04], ['grounded', 0.06], ['happen', 0.06], ['happening', 0.06], ['he', 0.04], ['high-energy', 0.06], ['history', 0.12], ['hostile', 0.06], ['hugo', 0.06], ['human', 0.06], ['hundred', 0.06], ['impressive', 0.06], ['instance', 0.06], ['intelligences', 0.06], ['interesting', 0.11], ['intermixing', 0.06], ['kept', 0.06], ['know', 0.06], ['last', 0.06], ['like', 0.06], ['long', 0.06], ['love', 0.06], ['made', 0.12], ['making', 0.06], ['many', 0.11], ['math', 0.06], ['message', 0.06], ['microcosmos', 0.06], ['middle', 0.06], ['million', 0.06], ['miniature', 0.06], ['monitoring', 0.06], ['most', 0.06], ['must', 0.06], ['my', 0.04], ['nature', 0.06], ['neutrons', 0.12], ['next', 0.06], ['on', 0.02], ['original', 0.06], ['part', 0.06], ['person', 0.04], ['physicists', 0.06], ['physics', 0.06], ['pretty', 0.06], ['problem', 0.12], ['progress', 0.06], ['protons', 0.12], ['pushes', 0.06], ['quote', 0.06], ['ray', 0.06], ['reaction', 0.06], ['reactionaries', 0.06], ['received', 0.06], ['revolutionary', 0.06], ['scientific', 0.06], ['second', 0.06], ['seti', 0.06], ['side', 0.06], ['slow', 0.06], ['smart', 0.06], ['smashed', 0.06], ['someone', 0.06], ['special', 0.06], ['started', 0.12], ['stations', 0.06], ['still', 0.06], ['stuff', 0.06], ['such', 0.06], ['tell', 0.06], ['than', 0.06], ['them', 0.06], ['then', 0.12], ['there', 0.02], ['things', 0.12], ['thinking', 0.06], ['third', 0.19], ['this', 0.06], ['thousands', 0.06], ['through', 0.19], ['together', 0.06], ['translated', 0.06], ['understand', 0.06], ['universes', 0.12], ['unsolvable', 0.06], ['unveiled', 0.06], ['up', 0.06], ['what', 0.06], ['where', 0.07], ['which', 0.04], ['who', 0.07], ['will', 0.04], ['winner', 0.06], ['your', 0.06]]\n",
            "[[\"'ve\", 0.04], [',', 0.04], ['by', 0.02], ['have', 0.09], ['if', 0.02], ['is', 0.04], ['me', 0.08], [\"n't\", 0.04], ['superior', 0.08], ['was', 0.08], ['did', 0.08], ['for', 0.04], ['good', 0.08], ['he', 0.08], ['like', 0.04], ['on', 0.04], ['there', 0.04], ['this', 0.04], ['who', 0.08], ['!', 0.14], [\"'m\", 0.14], ['are', 0.08], ['both', 0.14], ['curious', 0.08], ['detectives', 0.14], ['disappoint', 0.14], ['ever', 0.14], ['ex-military', 0.14], ['fast', 0.08], ['france', 0.04], ['french', 0.08], ['fun', 0.09], ['holiday', 0.14], ['inspired', 0.14], ['looking', 0.14], ['mysteries', 0.14], ['mystery', 0.28], ['not', 0.14], ['one', 0.08], ['paced', 0.08], ['poirot', 0.42], ['seems', 0.14], ['set', 0.08], ['sherlock', 0.42], ['sidekick', 0.14], ['similar', 0.14], ['solve', 0.14], ['story', 0.14], ['struck', 0.14], ['sure', 0.14], ['telling', 0.14], ['their', 0.14], ['using', 0.14], ['while', 0.09], ['wit', 0.14]]\n",
            "[[',', 0.06], ['-', 0.05], ['be', 0.03], ['but', 0.03], ['dark', 0.03], ['descriptions', 0.05], ['if', 0.01], ['is', 0.01], ['it', 0.09], ['its', 0.03], ['more', 0.06], ['much', 0.05], [\"n't\", 0.03], ['of', 0.08], ['once', 0.03], ['out', 0.03], ['right', 0.05], ['so', 0.01], ['think', 0.06], ['though', 0.06], ['was', 0.08], ['we', 0.03], ['were', 0.03], ['you', 0.03], ['3', 0.05], [':', 0.05], ['about', 0.03], ['at', 0.05], ['different', 0.16], ['do', 0.1], ['etc', 0.05], ['first', 0.05], ['game', 0.05], ['got', 0.05], ['interesting', 0.05], ['like', 0.03], ['my', 0.05], ['person', 0.05], ['there', 0.03], ['this', 0.09], ['what', 0.03], ['where', 0.05], ['which', 0.05], ['will', 0.05], ['are', 0.1], ['curious', 0.05], ['fast', 0.05], ['france', 0.03], ['fun', 0.06], ['paced', 0.05], ['set', 0.05], ['while', 0.03], ['(', 0.36], [')', 0.27], ['30', 0.18], ['accounts', 0.09], ['after', 0.09], ['attention', 0.18], ['beach', 0.09], ['bought', 0.09], ['captured', 0.09], ['case', 0.09], ['certainly', 0.09], ['climax', 0.09], ['cultivate', 0.09], ['distilled', 0.09], ['disturbing', 0.09], ['enjoyed', 0.09], ['forewarned', 0.09], ['general', 0.09], ['generally', 0.09], ['great', 0.05], ['gripping', 0.09], ['guess', 0.09], ['had', 0.16], ['hearing', 0.09], ['holds', 0.09], ['kinds', 0.09], ['learning', 0.09], ['materials', 0.09], ['murder', 0.09], ['murderers', 0.09], ['nose', 0.09], ['now', 0.09], ['oils', 0.09], ['over', 0.09], ['pay', 0.18], ['perfume', 0.09], ['power', 0.18], ['pun', 0.09], ['reading', 0.18], ['ridiculous', 0.09], ['scent', 0.18], ['scents', 0.09], ['situations', 0.09], ['store', 0.09], ['stronger', 0.09], ['tale', 0.09], ['twisted', 0.09], ['us', 0.09], ['use', 0.09], ['very', 0.09], ['ways', 0.09], ['wife', 0.09], ['wine', 0.18], ['wish', 0.09], ['worth', 0.09], ['yes', 0.09]]\n",
            "[['-', 0.02], ['as', 0.05], ['by', 0.02], ['every', 0.05], ['loved', 0.05], ['no', 0.05], ['of', 0.05], ['once', 0.05], ['or', 0.09], ['so', 0.02], ['were', 0.05], ['you', 0.05], ['about', 0.1], ['an', 0.09], ['culture', 0.18], ['for', 0.05], ['many', 0.09], ['on', 0.05], ['what', 0.05], ['france', 0.05], ['french', 0.28], ['fun', 0.05], ['one', 0.18], ['while', 0.05], ['great', 0.09], ['had', 0.18], ['annecdotes', 0.16], ['appreciate', 0.16], ['author', 0.16], ['codes', 0.16], ['comparing', 0.16], ['day', 0.16], ['definitely', 0.16], ['eg', 0.16], ['error', 0.16], ['expat', 0.16], ['fax', 0.16], ['gives', 0.16], ['gym', 0.16], ['hilarious', 0.16], ['listening', 0.16], ['living', 0.16], ['narrated', 0.16], ['only', 0.16], ['paris', 0.16], ['plan', 0.32], ['recommend', 0.16], ['sense', 0.16], ['vacation', 0.16], ['visiting', 0.16], ['week', 0.16]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra4TgxEbWf6X",
        "outputId": "57e6d68d-7a53-445b-f104-0f098e1a22ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " # Create similarity object. The main class is Similarity, which builds an index for a given set of documents\n",
        "\n",
        "# building the index\n",
        "# sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
        "#                                        num_features=len(dictionary))\n",
        "sims = gensim.similarities.Similarity('',tf_idf[corpus],\n",
        "                                        num_features=len(dictionary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbIeyk4bW4Wv",
        "outputId": "064435c4-58bd-4361-b4f2-0c2bb218d38c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create Query Document\n",
        "\n",
        "# Once the index is built, we are going to calculate how similar is \n",
        "# this query document to each document in the index\n",
        "\n",
        "# query document is the original Amazon review\n",
        "file2_docs = []\n",
        "\n",
        "line = sent_tokenize(first_review_text)\n",
        "file2_docs.append(str(line))\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs)) \n",
        "print(file2_docs) \n",
        "\n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    #update an existing dictionary and create bag of words\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 1\n",
            "[\"['Thoroughly good read, gives the perspective of the war from many angles, especially the impact on children.']\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmvz7ej6aQ06",
        "outputId": "dbf0c02b-c984-465c-c80a-ae5c1d854e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# perform a similarity query against the corpus\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "# print(document_number, document_similarity)\n",
        "print('Comparing Result:', sims[query_doc_tf_idf]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comparing Result: [0.06128649 0.12527286 0.03558587 0.01429004 0.12113652]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}